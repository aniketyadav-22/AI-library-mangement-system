# -*- coding: utf-8 -*-
"""book_recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ajkOshKZ8cs_lj3qk_AGc6drA9C-0Fln


"""

# !pip install keybert


import pandas as pd
import numpy as np


import re
import string


from nltk.classify.textcat import TextCat


import dask.dataframe as dd
import multiprocessing


from keybert import KeyBERT


from sklearn.feature_extraction.text import TfidfVectorizer


import matplotlib.pyplot as plt
import altair as alt
alt.renderers.enable('mimetype')


from sklearn.metrics.pairwise import cosine_similarity

min_description_word_count = 3

books_data = pd.read_csv("book1000k-1100k.csv", usecols=['Id', 'Name', 'Authors', 'ISBN', 'PublishYear', 'Publisher', 'Language', 'Description'])
print(books_data.shape)
books_data.head(5)

books_data.dropna(subset=["Description"], inplace=True)

list(books_data.Description[books_data.Id == 1099555]) #Description with url and html tag

url_pattern = re.compile(r'https?://\S+|www\.\S+')
def remove_url(text):
    return re.sub(url_pattern, r'', text)

html_pattern = re.compile('<[^>]*>')
def clean_html_tags(text):
    return re.sub(html_pattern, r'', text)

punctuations = string.punctuation
def remove_punctuations(text):
    return text.translate(str.maketrans('', '', punctuations))

books_data.Description = books_data.Description.apply(remove_url)
books_data.Description = books_data.Description.apply(clean_html_tags)
books_data.Description = books_data.Description.apply(remove_punctuations)


list(books_data.Description[books_data.Id == 1099555])

books_data[["Publisher"]] = books_data[["Publisher"]].fillna("unknown")
books_data[["Name", "Authors", "Publisher", "Description"]] = pd.concat([books_data[col].astype(str).str.lower().str.strip()
                                                                             for col in ["Name", "Authors", "Publisher", "Description"]],
                                                                            axis=1)

books_data["length"] = [len(d.split()) for d in books_data['Description'].tolist()]

print(set(books_data.Description[books_data.length.isin(range(0,4))]))

books_data.Description = books_data.Description.replace(r'^\s*$', np.nan, regex=True)

books_data[books_data.length.isin(range(1,min_description_word_count+1))][["Id", "Name", "Description", "length"]]\
.sort_values(by=["length"], ascending=True).head(5)

books_data.dropna(subset=["Description"], inplace=True)

books_data.drop(books_data.index[books_data.length.isin(range(0,min_description_word_count+1))], inplace = True)
del books_data["length"]

books_data["Publisher"] = books_data.Publisher.replace('unknown',np.nan)
books_data = books_data.sort_values(by="Publisher", na_position='last')\
.drop_duplicates(subset=["Name", "Authors", "Description"], keep='first')

series_pattern =  "(?:[;]\s*|\(\s*)([^\(;]*\s*#\s*\d+(?:\.?\d+|\\&\d+|-?\d*))"
def get_book_series_info(text):
    series_info = re.findall(series_pattern, text)
    if series_info:
        series_info = " ".join([i.replace(" ", "_") for i in series_info])
        return series_info
    else:
        return np.nan

books_data['BookSeriesInfo'] = books_data.Name.apply(get_book_series_info)

series_remove_pattern = re.compile("(?:[\(]\s*[^\(;]*\s*#\s*\d+(?:\.?\d+|\\&\d+|-?\d*)(?:;|\))|\s*[^\(;]*\s*#\s*\d+(?:\.?\d+|\\&\d+|-?\d*)\))")
def remove_series_info(text):
    return re.sub(series_remove_pattern, r'', text)

books_data["Title"]= books_data["Name"].str.replace(series_remove_pattern, r'',regex=True).str.strip()

import nltk
nltk.download('crubadan')

tc = TextCat()

def detect_language(text):
    if pd.isna(text):
        return 'eng'
    text = " ".join(text.split()[:5])

    if text.isnumeric():
        return 'eng'
    else:
        return tc.guess_language(text).strip()

import nltk
nltk.download('punkt_tab')

temp_preview = books_data.head(5).copy()
ddf = dd.from_pandas(temp_preview, npartitions=4*multiprocessing.cpu_count())
temp_preview["Language"] = ddf.map_partitions(lambda df: df.apply(lambda x: detect_language(x['Name']) if pd.isna(x['Language']) else x['Language'], axis=1)).compute()
temp_preview

books_data["Publisher"] = books_data["Publisher"].str.replace('"','')

books_data["Authors"] = books_data["Authors"].str.strip().str.replace(' ','_')
books_data["Publisher"] = books_data["Publisher"].str.strip().str.replace(' ','_')
books_data.head(5)

books_data["bow"] = books_data[["BookSeriesInfo", 'Authors', 'Publisher', 'Language']].fillna('').agg(' '.join, axis=1)

books_data.bow.iloc[8375]

books_data.to_csv("preprocessed.csv", sep=",", index=False)

fe_data = pd.read_csv("preprocessed.csv", usecols=["Id", "Name", "Language", "Description", "bow"])
fe_data.head()

kw_model = KeyBERT()

from tqdm.auto import tqdm
tqdm.pandas()

def get_keywords(text):
    if pd.isna(text) or not isinstance(text, str):
        return ""
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words="english")
    return " ".join([kw[0] for kw in keywords])


fe_data_subset = fe_data.head(1000)
fe_data["keywords"] = fe_data_subset["Description"].progress_apply(get_keywords)

fe_data["keywords"].head()

fe_data["keywords"] = fe_data[['bow', 'keywords']].fillna('').agg(' '.join, axis=1)
fe_data.drop(['bow', 'Description'], axis = 1, inplace=True)

fe_data = fe_data.drop_duplicates(subset=["Name"], keep='first')

fe_data.to_csv("keywords.csv", sep=",", index=False)

model_data = pd.read_csv("keywords.csv")
model_data.head()

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(analyzer = 'word',
                        min_df=3,
                        max_df = 0.6,
                        stop_words="english",
                        encoding = 'utf-8',
                        token_pattern=r"(?u)\S\S+")
tfidf_encoding = tfidf.fit_transform(model_data["keywords"])

print(tfidf.get_feature_names_out()[1:100])



tfidf_df = pd.DataFrame(tfidf_encoding.toarray(), index=model_data["Name"], columns=tfidf.get_feature_names_out())

tfidf_df["total"]= tfidf_df.sum(axis=1)
tfidf_df = tfidf_df.sort_values("total", ascending=False)
del tfidf_df["total"]

tfidf_df_preview = tfidf_df.iloc[100:150,25:].copy()
tfidf_df_preview = tfidf_df_preview.stack().reset_index()
tfidf_df_preview = tfidf_df_preview.rename(columns={0:'tfidf', 'Name': 'book','level_1': 'term'})
tfidf_df_preview = tfidf_df_preview.sort_values(by=['book','tfidf'], ascending=[True,False]).groupby(['book']).head(10)
# disp(tfidf_df_preview)

def process_word_matrix(word_vec):

    word_vec.term = word_vec.term.str.replace('_',' ')


    word_vec = word_vec[word_vec.tfidf > 0]

    return word_vec

tfidf_vec = process_word_matrix(tfidf_df_preview.copy())
tfidf_vec.iloc[0:5]


import altair as alt
grid = alt.Chart(tfidf_vec).encode(
    x = 'rank:O',
    y = 'book:N'
).transform_window(
    rank = "dense_rank()",
    sort = [alt.SortField("tfidf", order="descending")],
    groupby = ["book"],
)
heatmap = grid.mark_rect(size=5).encode(
    alt.Color('tfidf:Q', scale=alt.Scale(scheme='redpurple'))
)
text = grid.mark_text(align='center', baseline='middle', lineBreak='').encode(
    text = 'term:N',
    color = alt.condition(alt.datum.tfidf >= 0.23, alt.value('white'), alt.value('black'))
)
(heatmap+text).properties(width = 800)

book_cosine_sim = cosine_similarity(tfidf_encoding, tfidf_encoding)

book_cosine_sim

plt.figure(figsize=(6, 6), dpi=80)
# plt.spy(book_cosine_sim, precision = 0.1, markersize = 0.04)
plt.tight_layout()
plt.show()


books = pd.Series(model_data['Name'])

def recommend_books_similar_to(book_name, n=5, cosine_sim_mat=book_cosine_sim):
    input_idx = books[books == book_name].index[0]
    top_n_books_idx = list(pd.Series(cosine_sim_mat[input_idx]).sort_values(ascending = False).iloc[1:n+1].index)

    books_list = list(books)
    recommended_books = [books[i] for i in top_n_books_idx]

    return recommended_books




print("\033[1m{}\033[0m".format("Recommendation (Series Information) based on the read: The Eastland Disaster (Images of America: Illinois)"))
print(recommend_books_similar_to("the eastland disaster (images of america: illinois)", 5))



print("\n\033[1m{}\033[0m".format("Recommendation (Numbered Series) based on the read: The Majolica Murders (Antique Lover, #5)"))
print(recommend_books_similar_to("the majolica murders (antique lover, #5)", 5))


print("\n\033[1m{}\033[0m".format("Recommendation (Theme: Programming) based on the read: The Practice of Programming (Addison-Wesley Professional Computing Series)"))
print(recommend_books_similar_to('the practice of programming (addison-wesley professional computing series)', 5))



print("\n\033[1m{}\033[0m".format("Recommendation (Author: Dean Koontz) based on the read: Cold Fire"))
print(recommend_books_similar_to("cold fire",5))